{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iijXHod-j0O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "def missing_values_analysis(df):\n",
        "    \"\"\"Calculate missing values percentage\"\"\"\n",
        "    missing = df.isnull().sum()\n",
        "    missing_percent = (missing / len(df)) * 100\n",
        "    missing_values = pd.DataFrame({'Missing Count': missing, 'Missing Percentage': missing_percent})\n",
        "    return missing_values[missing_values['Missing Count'] > 0].sort_values('Missing Percentage', ascending=False)\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \"\"\"Handle missing values based on data description\"\"\"\n",
        "    # Numerical features to impute with median\n",
        "    numerical_features = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
        "\n",
        "    # Categorical features to impute with 'None'\n",
        "    categorical_none = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
        "                       'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
        "                       'PoolQC', 'Fence', 'MiscFeature']\n",
        "\n",
        "    # Handle numerical missing values\n",
        "    for feature in numerical_features:\n",
        "        df[feature] = df[feature].fillna(df[feature].median())\n",
        "\n",
        "    # Handle categorical missing values\n",
        "    for feature in categorical_none:\n",
        "        df[feature] = df[feature].fillna('None')\n",
        "\n",
        "    # Fill remaining categorical variables with mode\n",
        "    categorical_mode = df.select_dtypes(include=['object']).columns\n",
        "    for feature in categorical_mode:\n",
        "        if feature not in categorical_none and feature != 'is_train':\n",
        "            df[feature] = df[feature].fillna(df[feature].mode()[0])\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_correlations(df, threshold=0.7):\n",
        "    \"\"\"Analyze and visualize correlations between features\"\"\"\n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = df.select_dtypes(include=['int64', 'float64']).corr()\n",
        "\n",
        "    # Find highly correlated feature pairs\n",
        "    high_correlations = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "                high_correlations.append({\n",
        "                    'feature1': correlation_matrix.columns[i],\n",
        "                    'feature2': correlation_matrix.columns[j],\n",
        "                    'correlation': correlation_matrix.iloc[i, j]\n",
        "                })\n",
        "\n",
        "    # Sort by absolute correlation value\n",
        "    high_correlations = sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)\n",
        "\n",
        "    # Create correlation plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'correlation_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return high_correlations\n",
        "\n",
        "def handle_collinearity(df, high_correlations, target_column='SalePrice'):\n",
        "    \"\"\"Handle highly correlated features\"\"\"\n",
        "    features_to_drop = set()\n",
        "\n",
        "    print(\"\\nHighly correlated feature pairs:\")\n",
        "    print(\"Feature 1 | Feature 2 | Correlation\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for corr in high_correlations:\n",
        "        print(f\"{corr['feature1']} | {corr['feature2']} | {corr['correlation']:.3f}\")\n",
        "\n",
        "        # If one feature is already marked for removal, skip\n",
        "        if corr['feature1'] in features_to_drop or corr['feature2'] in features_to_drop:\n",
        "            continue\n",
        "\n",
        "        # If dealing with training data and target column exists\n",
        "        if target_column in df.columns:\n",
        "            # Calculate correlation with target for both features\n",
        "            corr1 = abs(df[corr['feature1']].corr(df[target_column]))\n",
        "            corr2 = abs(df[corr['feature2']].corr(df[target_column]))\n",
        "\n",
        "            # Keep the feature with higher correlation to target\n",
        "            if corr1 < corr2:\n",
        "                features_to_drop.add(corr['feature1'])\n",
        "            else:\n",
        "                features_to_drop.add(corr['feature2'])\n",
        "        else:\n",
        "            # If no target column, drop the second feature by default\n",
        "            features_to_drop.add(corr['feature2'])\n",
        "\n",
        "    print(f\"\\nFeatures to be removed due to high correlation: {features_to_drop}\")\n",
        "\n",
        "    # Drop highly correlated features\n",
        "    df = df.drop(columns=list(features_to_drop))\n",
        "\n",
        "    return df\n",
        "\n",
        "def engineer_features(df):\n",
        "    \"\"\"Create new features\"\"\"\n",
        "    # Total square footage\n",
        "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
        "\n",
        "    # Total bathrooms\n",
        "    df['TotalBathrooms'] = df['FullBath'] + (0.5 * df['HalfBath']) + \\\n",
        "                          df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath'])\n",
        "\n",
        "    # House age and remodeled\n",
        "    df['Age'] = df['YrSold'] - df['YearBuilt']\n",
        "    df['IsRemodeled'] = (df['YearRemodAdd'] != df['YearBuilt']).astype(int)\n",
        "    df['RemodAge'] = df['YrSold'] - df['YearRemodAdd']\n",
        "\n",
        "    # Total porch area\n",
        "    df['TotalPorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + \\\n",
        "                         df['3SsnPorch'] + df['ScreenPorch']\n",
        "\n",
        "    # Has features\n",
        "    df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n",
        "    df['HasGarage'] = (df['GarageArea'] > 0).astype(int)\n",
        "    df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n",
        "\n",
        "    # Overall quality score\n",
        "    df['QualityScore'] = df['OverallQual'] * df['OverallCond']\n",
        "\n",
        "    return df\n",
        "\n",
        "def encode_categorical_features(df):\n",
        "    \"\"\"Encode categorical features\"\"\"\n",
        "    # Initialize label encoder\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Get categorical columns\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Encode each categorical feature\n",
        "    for feature in categorical_features:\n",
        "        if feature != 'is_train':\n",
        "            df[feature] = le.fit_transform(df[feature].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "def scale_features(df):\n",
        "    \"\"\"Scale numerical features\"\"\"\n",
        "    # Initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Get numerical columns\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Drop 'is_train' if it exists\n",
        "    if 'is_train' in numerical_features:\n",
        "        numerical_features = numerical_features.drop('is_train')\n",
        "\n",
        "    # Scale numerical features\n",
        "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    # Get the current directory\n",
        "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    data_dir = os.path.join(parent_dir, 'house-prices-advanced-regression-techniques')\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    train_data = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "    test_data = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
        "\n",
        "    # Create train/test flag\n",
        "    train_data['is_train'] = 1\n",
        "    test_data['is_train'] = 0\n",
        "\n",
        "    # Combine datasets\n",
        "    all_data = pd.concat([train_data, test_data], axis=0, sort=False)\n",
        "    print(f\"Total samples: {len(all_data)}\")\n",
        "    print(f\"Training samples: {len(train_data)}\")\n",
        "    print(f\"Test samples: {len(test_data)}\")\n",
        "\n",
        "    # Save original SalePrice for training data\n",
        "    train_sale_price = train_data['SalePrice'].copy() if 'SalePrice' in train_data.columns else None\n",
        "\n",
        "    # Analyze missing values\n",
        "    print(\"\\nAnalyzing missing values...\")\n",
        "    print(missing_values_analysis(all_data))\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    all_data = handle_missing_values(all_data)\n",
        "\n",
        "    # Engineer features\n",
        "    print(\"\\nEngineering new features...\")\n",
        "    all_data = engineer_features(all_data)\n",
        "\n",
        "    # Encode categorical features\n",
        "    print(\"\\nEncoding categorical features...\")\n",
        "    all_data = encode_categorical_features(all_data)\n",
        "\n",
        "    # Analyze correlations\n",
        "    print(\"\\nAnalyzing feature correlations...\")\n",
        "    high_correlations = analyze_correlations(all_data, threshold=0.7)\n",
        "\n",
        "    # Handle collinearity\n",
        "    print(\"\\nHandling multicollinearity...\")\n",
        "    all_data = handle_collinearity(all_data, high_correlations)\n",
        "\n",
        "    # Scale features\n",
        "    print(\"\\nScaling features...\")\n",
        "    all_data = scale_features(all_data)\n",
        "\n",
        "    # Split back into train and test\n",
        "    processed_train = all_data[all_data['is_train'] == 1].drop('is_train', axis=1)\n",
        "    processed_test = all_data[all_data['is_train'] == 0].drop('is_train', axis=1)\n",
        "\n",
        "    # Add back SalePrice to training data\n",
        "    if train_sale_price is not None:\n",
        "        processed_train['SalePrice'] = train_sale_price\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = os.path.join(current_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Export processed datasets\n",
        "    print(\"\\nExporting processed datasets...\")\n",
        "    processed_train.to_csv(os.path.join(output_dir, 'processed_train.csv'), index=False)\n",
        "    processed_test.to_csv(os.path.join(output_dir, 'processed_test.csv'), index=False)\n",
        "\n",
        "    print(f\"\\nProcessed training data shape: {processed_train.shape}\")\n",
        "    print(f\"Processed test data shape: {processed_test.shape}\")\n",
        "    print(\"\\nPreprocessing completed! Files saved as 'processed_train.csv' and 'processed_test.csv'\")\n",
        "    print(\"Correlation matrix visualization saved as 'correlation_matrix.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UlIXbnhg--Ew"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "E4m194nK_BMk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_values_analysis(df):\n",
        "    \"\"\"Calculate missing values percentage\"\"\"\n",
        "    missing = df.isnull().sum()\n",
        "    missing_percent = (missing / len(df)) * 100\n",
        "    missing_values = pd.DataFrame({'Missing Count': missing, 'Missing Percentage': missing_percent})\n",
        "    return missing_values[missing_values['Missing Count'] > 0].sort_values('Missing Percentage', ascending=False)"
      ],
      "metadata": {
        "id": "fTlgtnI1_GHo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_missing_values(df):\n",
        "    \"\"\"Handle missing values based on data description\"\"\"\n",
        "    # Numerical features to impute with median\n",
        "    numerical_features = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
        "\n",
        "    # Categorical features to impute with 'None'\n",
        "    categorical_none = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
        "                       'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
        "                       'PoolQC', 'Fence', 'MiscFeature']\n",
        "\n",
        "    # Handle numerical missing values\n",
        "    for feature in numerical_features:\n",
        "        df[feature] = df[feature].fillna(df[feature].median())\n",
        "\n",
        "    # Handle categorical missing values\n",
        "    for feature in categorical_none:\n",
        "        df[feature] = df[feature].fillna('None')\n",
        "\n",
        "    # Fill remaining categorical variables with mode\n",
        "    categorical_mode = df.select_dtypes(include=['object']).columns\n",
        "    for feature in categorical_mode:\n",
        "        if feature not in categorical_none and feature != 'is_train':\n",
        "            df[feature] = df[feature].fillna(df[feature].mode()[0])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "qlzX4h37_PEF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_correlations(df, threshold=0.7):\n",
        "    \"\"\"Analyze and visualize correlations between features\"\"\"\n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = df.select_dtypes(include=['int64', 'float64']).corr()\n",
        "\n",
        "    # Find highly correlated feature pairs\n",
        "    high_correlations = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "                high_correlations.append({\n",
        "                    'feature1': correlation_matrix.columns[i],\n",
        "                    'feature2': correlation_matrix.columns[j],\n",
        "                    'correlation': correlation_matrix.iloc[i, j]\n",
        "                })\n",
        "\n",
        "    # Sort by absolute correlation value\n",
        "    high_correlations = sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)\n",
        "\n",
        "    # Create correlation plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'correlation_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return high_correlations"
      ],
      "metadata": {
        "id": "SrldZhyA_buD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_collinearity(df, high_correlations, target_column='SalePrice'):\n",
        "    \"\"\"Handle highly correlated features\"\"\"\n",
        "    features_to_drop = set()\n",
        "\n",
        "    print(\"\\nHighly correlated feature pairs:\")\n",
        "    print(\"Feature 1 | Feature 2 | Correlation\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for corr in high_correlations:\n",
        "        print(f\"{corr['feature1']} | {corr['feature2']} | {corr['correlation']:.3f}\")\n",
        "\n",
        "        # If one feature is already marked for removal, skip\n",
        "        if corr['feature1'] in features_to_drop or corr['feature2'] in features_to_drop:\n",
        "            continue\n",
        "\n",
        "        # If dealing with training data and target column exists\n",
        "        if target_column in df.columns:\n",
        "            # Calculate correlation with target for both features\n",
        "            corr1 = abs(df[corr['feature1']].corr(df[target_column]))\n",
        "            corr2 = abs(df[corr['feature2']].corr(df[target_column]))\n",
        "\n",
        "            # Keep the feature with higher correlation to target\n",
        "            if corr1 < corr2:\n",
        "                features_to_drop.add(corr['feature1'])\n",
        "            else:\n",
        "                features_to_drop.add(corr['feature2'])\n",
        "        else:\n",
        "            # If no target column, drop the second feature by default\n",
        "            features_to_drop.add(corr['feature2'])\n",
        "\n",
        "    print(f\"\\nFeatures to be removed due to high correlation: {features_to_drop}\")\n",
        "\n",
        "    # Drop highly correlated features\n",
        "    df = df.drop(columns=list(features_to_drop))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "lJnD9MB7_1PF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df):\n",
        "    \"\"\"Create new features\"\"\"\n",
        "    # Total square footage\n",
        "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
        "\n",
        "    # Total bathrooms\n",
        "    df['TotalBathrooms'] = df['FullBath'] + (0.5 * df['HalfBath']) + \\\n",
        "                          df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath'])\n",
        "\n",
        "    # House age and remodeled\n",
        "    df['Age'] = df['YrSold'] - df['YearBuilt']\n",
        "    df['IsRemodeled'] = (df['YearRemodAdd'] != df['YearBuilt']).astype(int)\n",
        "    df['RemodAge'] = df['YrSold'] - df['YearRemodAdd']\n",
        "\n",
        "    # Total porch area\n",
        "    df['TotalPorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + \\\n",
        "                         df['3SsnPorch'] + df['ScreenPorch']\n",
        "\n",
        "    # Has features\n",
        "    df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n",
        "    df['HasGarage'] = (df['GarageArea'] > 0).astype(int)\n",
        "    df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n",
        "\n",
        "    # Overall quality score\n",
        "    df['QualityScore'] = df['OverallQual'] * df['OverallCond']\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "XwYpheFz_-Nn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_categorical_features(df):\n",
        "    \"\"\"Encode categorical features\"\"\"\n",
        "    # Initialize label encoder\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Get categorical columns\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Encode each categorical feature\n",
        "    for feature in categorical_features:\n",
        "        if feature != 'is_train':\n",
        "            df[feature] = le.fit_transform(df[feature].astype(str))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "YJtooJHZADuW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_features(df):\n",
        "    \"\"\"Scale numerical features\"\"\"\n",
        "    # Initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Get numerical columns\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Drop 'is_train' if it exists\n",
        "    if 'is_train' in numerical_features:\n",
        "        numerical_features = numerical_features.drop('is_train')\n",
        "\n",
        "    # Scale numerical features\n",
        "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "jMzS5qOjAJRv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   # Get the current directory\n",
        "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    data_dir = os.path.join(parent_dir, 'house-prices-advanced-regression-techniques')\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    train_data = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "    test_data = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
        "\n",
        "    # Create train/test flag\n",
        "    train_data['is_train'] = 1\n",
        "    test_data['is_train'] = 0\n",
        "\n",
        "    # Combine datasets\n",
        "    all_data = pd.concat([train_data, test_data], axis=0, sort=False)\n",
        "    print(f\"Total samples: {len(all_data)}\")\n",
        "    print(f\"Training samples: {len(train_data)}\")\n",
        "    print(f\"Test samples: {len(test_data)}\")\n",
        "\n",
        "    # Save original SalePrice for training data\n",
        "    train_sale_price = train_data['SalePrice'].copy() if 'SalePrice' in train_data.columns else None\n",
        "\n",
        "    # Analyze missing values\n",
        "    print(\"\\nAnalyzing missing values...\")\n",
        "    print(missing_values_analysis(all_data))\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    all_data = handle_missing_values(all_data)\n",
        "\n",
        "    # Engineer features\n",
        "    print(\"\\nEngineering new features...\")\n",
        "    all_data = engineer_features(all_data)\n",
        "\n",
        "    # Encode categorical features\n",
        "    print(\"\\nEncoding categorical features...\")\n",
        "    all_data = encode_categorical_features(all_data)\n",
        "\n",
        "    # Analyze correlations\n",
        "    print(\"\\nAnalyzing feature correlations...\")\n",
        "    high_correlations = analyze_correlations(all_data, threshold=0.7)\n",
        "\n",
        "    # Handle collinearity\n",
        "    print(\"\\nHandling multicollinearity...\")\n",
        "    all_data = handle_collinearity(all_data, high_correlations)\n",
        "\n",
        "    # Scale features\n",
        "    print(\"\\nScaling features...\")\n",
        "    all_data = scale_features(all_data)\n",
        "\n",
        "    # Split back into train and test\n",
        "    processed_train = all_data[all_data['is_train'] == 1].drop('is_train', axis=1)\n",
        "    processed_test = all_data[all_data['is_train'] == 0].drop('is_train', axis=1)\n",
        "\n",
        "    # Add back SalePrice to training data\n",
        "    if train_sale_price is not None:\n",
        "        processed_train['SalePrice'] = train_sale_price\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = os.path.join(current_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Export processed datasets\n",
        "    print(\"\\nExporting processed datasets...\")\n",
        "    processed_train.to_csv(os.path.join(output_dir, 'processed_train.csv'), index=False)\n",
        "    processed_test.to_csv(os.path.join(output_dir, 'processed_test.csv'), index=False)\n",
        "\n",
        "    print(f\"\\nProcessed training data shape: {processed_train.shape}\")\n",
        "    print(f\"Processed test data shape: {processed_test.shape}\")\n",
        "    print(\"\\nPreprocessing completed! Files saved as 'processed_train.csv' and 'processed_test.csv'\")\n",
        "    print(\"Correlation matrix visualization saved as 'correlation_matrix.png'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "7huu72BvASB3",
        "outputId": "474fd572-a38d-431d-8037-26eb1233dcce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-11-359c16549a95>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-359c16549a95>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    current_dir = os.path.dirname(os.path.abspath(__file__))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}